# -*- coding: utf-8 -*-
"""Lab_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gBVpsBaIiSJ1LhjxOwuH6S_ygKr9M2W3
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive/My Drive/Big Data -lab_3

"""## **Libraries ...**"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

"""### **Getting Started  ##**"""

titles = open('titles/1.txt','r').read().strip().splitlines()
links = open('links/1.txt','r').read().strip().splitlines()

"""## **Observing the data...**"""

print(titles[20:30])
print(links)
titles = np.array(titles)

"""## **Creating the Adjacency Matrix...**"""

A = np.zeros((len(titles)+1,len(titles)+1))
# an example :
#from the node
links = np.array(links)
links_as_int = []
for link in links :
  source, destination = map(int, link.split())
  links_as_int.append((source,destination))
#print(links_as_int)
#print(links_as_int[:][0])
#print(links_as_int[0][1])


for row in links_as_int:
  A[row[1],row[0]] = 1
#print(A[1680:1686, 1])

Final_A = A[1:,1:]

#print(Final_A)

non_zeros = np.where(Final_A != 0)
print(np.sum(Final_A[non_zeros]))
print(len(links_as_int))
#:)))) the number of the links in the sparse representation is equal to the sum of non-zero elements in my FInal_A

k_n_in = np.sum(Final_A, axis=1)
print(np.sort(k_n_in))
#k_n_in = np.sort(k_n_in)[::-1]
print(k_n_in)

k_n_out = np.sum(Final_A, axis=0)
print(np.sort(k_n_out))
#k_n_out = np.sort(k_n_out)[::-1]
print(k_n_out)

k_n_in_norm = k_n_in/sum(k_n_in)
k_n_out_norm = k_n_out/sum(k_n_out)
print(k_n_in_norm)
print(k_n_out_norm)


k_n_in_norm_sort = np.sort(k_n_in_norm)[::-1][0:5]
k_n_out_norm_sort = np.sort(k_n_out_norm)[::-1][0:5]
print(k_n_in_norm_sort)
print(k_n_out_norm_sort)

indices_in = np.argsort(k_n_in_norm)[::-1][0:5]
k_n_out_norm_not_sort = k_n_out_norm[indices_in]
print(k_n_out_norm_not_sort)

indices_out = np.argsort(k_n_out_norm)[::-1][0:5]
k_n_in_norm_not_sort = k_n_in_norm[indices_out]
print(k_n_in_norm_not_sort)


titles_in = titles[indices_in]
titles_out = titles[indices_out]
print(titles_in)
print(titles_out)

# Create data for the table
In_Degree = {
    "Title": titles_in,
    "In_Degree_Centrality": k_n_in_norm_sort,
    "Out_Degree_Centrality": k_n_out_norm_not_sort
}

# Create the DataFrame
table1 = pd.DataFrame(In_Degree)

# Display the table
print(table1)

Out_Degree = {
    "Title": titles_out,
    "In_Degree_Centrality": k_n_in_norm_not_sort,
    "Out_Degree_Centrality": k_n_out_norm_sort
}

table2 = pd.DataFrame(Out_Degree)

# Display the table
print(table2)

"""## **END OF TASK 1**"""



"""## **Now we begin task 2**"""

hub_matrix = Final_A.T@Final_A
auth_matrix = Final_A@Final_A.T

# Compute eigenvalues and eigenvectors for authority
auth_eigenvalues, auth_eigenvectors = np.linalg.eig(auth_matrix)
# Get the principal eigenvector (corresponding to the largest eigenvalue)
auth_centrality = auth_eigenvectors[:, np.argmax(auth_eigenvalues)]
# Normalize
auth_centrality = auth_centrality / np.sum(auth_centrality)



hub_eigenvalues, hub_eigenvectors = np.linalg.eig(hub_matrix)
hub_centrality = hub_eigenvectors[:, np.argmax(hub_eigenvalues)]
hub_centrality = hub_centrality / np.sum(hub_centrality)

auth_norm_sort = np.sort(auth_centrality)[::-1][0:5]
hub_norm_sort = np.sort(hub_centrality)[::-1][0:5]
print(auth_norm_sort)
print(hub_norm_sort)

hub_norm_not_sort = hub_centrality[np.argsort(auth_centrality)[::-1][0:5]]
auth_norm_not_sort = auth_centrality[np.argsort(hub_centrality)[::-1][0:5]]
print(hub_norm_not_sort)
print(auth_norm_not_sort)

titles_auth = titles[np.argsort(auth_centrality)[::-1][0:5]]
titles_hub = titles[np.argsort(hub_centrality)[::-1][0:5]]
print(titles_auth)
print(titles_hub)

hubs = {
    'Titles' : titles_hub,
    'Hub_Centrality' : hub_norm_sort,
    'Authority_Centrality' : auth_norm_not_sort
}
table3 = pd.DataFrame(hubs)
print(table3)

auth = {
    'Titles' : titles_auth,
    'Hub_Centrality' : hub_norm_not_sort,
    'Authority_Centrality' : auth_norm_sort
}
table4 = pd.DataFrame(auth)
print(table4)

"""## **End of task 2**"""



"""## **Task 3**"""

# Compute eigenvalues and eigenvectors for authority
A_eigenvalues, A_eigenvectors = np.linalg.eig(Final_A)
# Get the principal eigenvector (corresponding to the largest eigenvalue)
A_centrality = A_eigenvectors[:, np.argmax(A_eigenvalues)]
# Normalize
A_centrality = A_centrality / np.sum(A_centrality)



A_norm_sort = np.sort(A_centrality)[::-1][0:5]

print(A_norm_sort)

#A_norm_not_sort = hub_centrality[np.argsort(auth_centrality)[::-1][0:5]]
#auth_norm_not_sort = auth_centrality[np.argsort(hub_centrality)[::-1][0:5]]
#print(hub_norm_not_sort)
#print(auth_norm_not_sort)

titles_A = titles[np.argsort(A_centrality)[::-1][0:5]]
#titles_hub = titles[np.argsort(hub_centrality)[::-1][0:5]]
#print(titles_A)
#print(titles_hub)



As = {
    'Titles' : titles_A,
    'A_Centrality' : A_norm_sort,

}
table5 = pd.DataFrame(As)
print(table5)

"""## **End of Task 3**

Task 4
"""

# Step 1: Compute the largest eigenvalue of the adjacency matrix
eigenvalues, _ = np.linalg.eig(Final_A)
lambda_max = max(abs(eigenvalues))

# Step 2: Calculate alpha
alpha = 0.85 / lambda_max

# Step 3: Compute Katz centrality
I = np.eye(Final_A.shape[0])  # Identity matrix of the same size as Final_A
ones_vector = np.ones((Final_A.shape[0], 1))  # Column vector of ones
katz_centrality = np.linalg.inv(I - alpha * Final_A) @ ones_vector

# Step 4: Normalize the centrality scores
katz_centrality = katz_centrality.flatten()  # Convert to 1D array for easier manipulation
katz_centrality /= katz_centrality.sum()

# Step 5: Identify the top 5 articles
top_indices = np.argsort(katz_centrality)[::-1][:5]
top_katz_scores = katz_centrality[top_indices]
top_titles = titles[top_indices]

# Create and display the results in a DataFrame
katz_table = pd.DataFrame({
    'Titles': top_titles,
    'Katz_Centrality': top_katz_scores
})
print(katz_table)

"""End of task4

Task 5
"""

def compute_pagerank(A, alpha):
    N = A.shape[0]

    # Step 1: Create transition probability matrix
    P = A / np.sum(A, axis=0, where=(A != 0), keepdims=True)
    P = np.nan_to_num(P)  # Handle columns with all zeros (dangling nodes)

    # Step 2: Create Google matrix
    E = np.ones((N, N)) / N
    G = alpha * P + (1 - alpha) * E

    # Step 3: Compute PageRank
    I = np.eye(N)  # Identity matrix
    b = (1 - alpha) / N * np.ones(N)
    pagerank_scores = np.linalg.inv(I - alpha * P) @ b

    # Step 4: Normalize PageRank scores
    pagerank_scores /= pagerank_scores.sum()

    return pagerank_scores

# Compute PageRank for alpha = 0.3, 0.85, 0.99
alphas = [0.3, 0.85, 0.99]
pagerank_results = {}

for alpha in alphas:
    pagerank_scores = compute_pagerank(Final_A, alpha)
    top_indices_pr = np.argsort(pagerank_scores)[::-1][:5]
    pagerank_results[alpha] = {
        'Scores': pagerank_scores[top_indices_pr],
        'Titles': titles[top_indices_pr]
    }

# Display results for each alpha
for alpha, result in pagerank_results.items():
    print(f"\nTop 5 articles for alpha = {alpha}:")
    pagerank_table = pd.DataFrame({
        'Titles': result['Titles'],
        'PageRank_Score': result['Scores']
    })
    print(pagerank_table)

"""end of task 5

task 6
"""

def iterative_pagerank(A, alpha, max_iter=100):
    N = A.shape[0]

    # Step 1: Create transition probability matrix
    P = A / np.sum(A, axis=0, where=(A != 0), keepdims=True)
    P = np.nan_to_num(P)  # Handle dangling nodes

    # Step 2: Initialize PageRank scores
    pagerank_scores = np.ones(N) / N
    exact_scores = compute_pagerank(A, alpha)
    all_iterations = []

    # Step 3: Iterative computation
    for _ in range(max_iter):
        pagerank_scores = alpha * (P @ pagerank_scores) + (1 - alpha) / N
        pagerank_scores /= pagerank_scores.sum()  # Normalize
        all_iterations.append(pagerank_scores.copy())

    return pagerank_scores, exact_scores, np.array(all_iterations)

# Compute iterative PageRank for alpha = 0.85
alpha = 0.85
max_iter = 100
iterative_scores, exact_scores, all_iterations = iterative_pagerank(Final_A, alpha, max_iter)

# Top 3 articles based on exact PageRank
top_indices = np.argsort(exact_scores)[::-1][:3]
titles_top3 = titles[top_indices]

# Extract scores for the top 3 articles over iterations
scores_top3 = all_iterations[:, top_indices]

# Plot evolution of scores
plt.figure(figsize=(10, 6))
for i, idx in enumerate(top_indices):
    plt.plot(range(1, max_iter + 1), scores_top3[:, i], label=f"Iterative {titles[idx]}")
    plt.axhline(y=exact_scores[idx], color=f"C{i}", linestyle='--',
                label=f"Exact {titles[idx]}")

plt.title("PageRank Score Evolution for Top-3 Articles")
plt.xlabel("Iterations")
plt.ylabel("PageRank Score")
plt.legend()
plt.grid(True)
plt.show()

# Create table for top 3 articles
top3_scores_iterative = iterative_scores[top_indices]
top3_scores_exact = exact_scores[top_indices]

table_iterative_pagerank = pd.DataFrame({
    "Title": titles[top_indices],
    "Exact PageRank": top3_scores_exact,
    "Iterative PageRank (100th Iteration)": top3_scores_iterative
})

print("\nTop 3 Articles by PageRank (Exact vs Iterative at 100th Iteration):")
print(table_iterative_pagerank)

# Create tables for scores over iterations
all_iterations_df = pd.DataFrame(all_iterations[:, top_indices],
                                 columns=titles[top_indices],
                                 index=range(1, max_iter + 1))
all_iterations_df.index.name = "Iteration"

# Display top rows of the scores over iterations
print("\nPageRank Score Evolution for Top 3 Articles (First Few Iterations):")
print(all_iterations_df.head(10))

# Save scores evolution to a file (optional)
all_iterations_df.to_csv("pagerank_evolution_top3.csv", index=True)

# Generate final table output
table_final = pd.DataFrame({
    "Title": titles_top3,
    "Final Iterative PageRank": scores_top3[-1],
    "Exact PageRank": exact_scores[top_indices],
}).sort_values(by="Exact PageRank", ascending=False)

print("\nFinal PageRank Table for Top-3 Articles:")
print(table_final)

"""comment for task 7"""



"""test for task 6:"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Function to compute the transition matrix H
def compute_transition_matrix(A):
    # Normalize columns of A to create the transition probability matrix H
    N = A.shape[0]
    H = A / np.sum(A, axis=0)
    H[np.isnan(H)] = 1/N
    #H = np.nan_to_num(H)  # Replace NaN with 0 for dangling nodes
    return H

# Iterative PageRank computation
def iterative_pagerank(H, alpha, num_iterations=100):
    N = H.shape[0]
    teleport = (1 - alpha) / N
    G = alpha * H + teleport * np.ones((N, N))
    zeta = np.ones(N) / N  # Initial uniform distribution
    zeta_history = [zeta.copy()]

    for _ in range(num_iterations):
        zeta = G @ zeta
        zeta /= np.sum(zeta)  # Normalize
        zeta_history.append(zeta.copy())

    return np.array(zeta_history)

# Closed-form PageRank computation
def closed_form_pagerank(H, alpha):
    N = H.shape[0]
    teleport = (1 - alpha) / N
    G = alpha * H + teleport * np.ones((N, N))
    I = np.eye(N)
    zeta = np.linalg.inv(I - alpha * H) @ ((1 - alpha) / N * np.ones(N))
    return zeta / np.sum(zeta)  # Normalize

# Load your adjacency matrix and titles
# Replace 'Final_A' and 'titles' with your actual data
H = compute_transition_matrix(Final_A)
alpha = 0.85

# Iterative PageRank
num_iterations = 100
zeta_history = iterative_pagerank(H, alpha, num_iterations=num_iterations)

# Closed-form PageRank
zeta_exact = closed_form_pagerank(H, alpha)

# Specific iterations
iterations_of_interest = [1, 2, 5, 10, 100]

# Printing top 5 articles for each iteration
print(f"Top iterative PageRank, alpha = {alpha} PageRank, iteration 1")
for i in iterations_of_interest:
    zeta_current = zeta_history[i, :]
    top_indices = np.argsort(zeta_current)[::-1][:5]  # Top 5 indices for this iteration
    print(f"Top iterative PageRank, alpha = {alpha} PageRank, iteration {i}")
    for idx, title in zip(top_indices, titles[top_indices]):
        print(f"{title} {zeta_current[idx]:.6f}")
    print()  # Add an empty line for separation

# Plot the evolution of scores for the top 5 articles
plt.figure(figsize=(12, 8))
for idx in top_indices:
    # Plot iterative PageRank scores (solid lines)
    plt.plot(range(num_iterations + 1), zeta_history[:, idx], label=f"Article {titles[idx]} (Iterative)", linewidth=2)
    # Plot exact PageRank scores (dashed lines)
    plt.plot([0, num_iterations], [zeta_exact[idx]] * 2, linestyle='--', label=f"Article {titles[idx]} (Exact)")

plt.xlabel("Iteration")
plt.ylabel("PageRank Score")
plt.title("Evolution of PageRank Scores for Top 5 Articles")
plt.legend(title="Article", loc='upper left')
plt.grid(True)
plt.show()

"""end of test for task 6

task 8
"""